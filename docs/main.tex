\documentclass[twoside,final]{hcmut-report}

\usepackage[utf8]{inputenc}
\usepackage[T5]{fontenc}
\usepackage[protrusion=false]{microtype}

\usepackage{graphicx, caption}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{multirow,multicol}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{float}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{tabularx}
\usepackage{makecell}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{tcolorbox}
\usepackage{lastpage}

\definecolor{keywordcolor}{rgb}{0.13,0.13,1} % Blue keywords
\definecolor{stringcolor}{rgb}{0.7,0.13,0.13} % Dark red strings
\definecolor{commentcolor}{rgb}{0.25,0.5,0.35} % Green comments
\definecolor{backgroundcolor}{rgb}{0.97,0.97,0.97} % Light background
\definecolor{numbercolor}{rgb}{0.85,0.65,0.13} % Numbers

\lstdefinelanguage{Python}{
  morekeywords={
    and, as, assert, async, await, break, class, continue, def, del, elif, else, except,
    False, finally, for, from, global, if, import, in, is, lambda, None, nonlocal, not, 
    or, pass, raise, return, True, try, while, with, yield
  },
  keywordstyle=\color{keywordcolor}\bfseries,
  morekeywords=[2]{bool, bytes, bytearray, complex, dict, float, frozenset, int, list, object, set, str, tuple},
  keywordstyle=[2]\color{blue}\bfseries,
  identifierstyle=\color{black},
  sensitive=true,
  comment=[l]{\#},
  morecomment=[s]{'''}{'''}, 
  morecomment=[s]{"""}{"""},
  commentstyle=\color{commentcolor}\ttfamily,
  stringstyle=\color{stringcolor}\ttfamily,
  morestring=[b]',
  morestring=[b]",
  % Highlight numbers
  literate=
    *{0}{{{\color{numbercolor}0}}}1
    {1}{{{\color{numbercolor}1}}}1
    {2}{{{\color{numbercolor}2}}}1
    {3}{{{\color{numbercolor}3}}}1
    {4}{{{\color{numbercolor}4}}}1
    {5}{{{\color{numbercolor}5}}}1
    {6}{{{\color{numbercolor}6}}}1
    {7}{{{\color{numbercolor}7}}}1
    {8}{{{\color{numbercolor}8}}}1
    {9}{{{\color{numbercolor}9}}}1
    {.0}{{{\color{numbercolor}.0}}}2
    {.1}{{{\color{numbercolor}.1}}}2
    {.2}{{{\color{numbercolor}.2}}}2
    {.3}{{{\color{numbercolor}.3}}}2
    {.4}{{{\color{numbercolor}.4}}}2
    {.5}{{{\color{numbercolor}.5}}}2
    {.6}{{{\color{numbercolor}.6}}}2
    {.7}{{{\color{numbercolor}.7}}}2
    {.8}{{{\color{numbercolor}.8}}}2
    {.9}{{{\color{numbercolor}.9}}}2
    {e+}{{{\color{numbercolor}e+}}}2
    {e-}{{{\color{numbercolor}e-}}}2
    {\#\#NUMSTART\#\#}{}{0}
    {\#\#NUMEND\#\#}{}{0},
}
\lstset{
  language=Python,
  backgroundcolor=\color{backgroundcolor},
  basicstyle=\ttfamily\small,
  numbers=left,
  numberstyle=\tiny\color{gray},
  stepnumber=1,
  numbersep=10pt,
  tabsize=2,
  showspaces=false,
  showstringspaces=false,
  breaklines=true,
  breakatwhitespace=true,
  frame=single,
}

\lstdefinelanguage{TypeScript}{
  keywords={abstract, as, boolean, break, case, catch, class, continue, const, constructor, debugger, declare, default, delete, do, else, enum, export, extends, false, finally, for, from, function, get, if, implements, import, in, infer, instanceof, interface, is, keyof, let, module, namespace, never, new, null, number, object, of, package, private, protected, public, readonly, require, global, return, set, static, string, super, switch, symbol, this, throw, true, try, type, typeof, undefined, unique, unknown, var, void, while, with, yield, async, await},
  keywordstyle=\color{keywordcolor}\bfseries,
  ndkeywords={string, number, boolean, Promise, any, void},
  ndkeywordstyle=\color{blue}\bfseries,
  identifierstyle=\color{black},
  sensitive=true,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{commentcolor}\ttfamily,
  stringstyle=\color{stringcolor}\ttfamily,
  morestring=[b]',
  morestring=[b]",
}

\lstset{
  language=TypeScript,
  backgroundcolor=\color{backgroundcolor},
  basicstyle=\ttfamily\small,
  numbers=left,
  numberstyle=\tiny\color{gray},
  stepnumber=1,
  numbersep=10pt,
  tabsize=2,
  showspaces=false,
  showstringspaces=false,
  breaklines=true,
  breakatwhitespace=true,
  frame=single,
}
\lstdefinelanguage{EnvCustom}{
  keywords={VITE_APP_API_URL, DATABASE_URL, DB_HOST, DB_USER, DB_PASSWORD, DB_PORT, DB_NAME, JWT_SECRET},
  keywordstyle=\color{blue}\bfseries,
  sensitive=true,
  comment=[l]{\#},
  commentstyle=\color{gray}\ttfamily,
  morestring=[b]',
  morestring=[b]",
  stringstyle=\color{red}\ttfamily,
  identifierstyle=\color{black}
}

\lstdefinelanguage{JavaScriptCustom}{
  keywords={
    break, case, catch, class, const, continue, debugger, default, delete, do, else, export,
    extends, finally, for, function, if, import, in, instanceof, let, new, return, super, 
    switch, this, throw, try, typeof, var, void, while, with, yield, await, async,
    mysql, dotenv, process, config, createPool, getConnection, release
  },
  keywordstyle=\color{blue}\bfseries,
  ndkeywords={true, false, null, undefined, NaN, Infinity},
  ndkeywordstyle=\color{teal}\bfseries,
  identifierstyle=\color{black},
  sensitive=true,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{gray}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]"
}


\lstdefinelanguage{SQL}
{
  keywords={SELECT, FROM, WHERE, GROUP BY, HAVING, ORDER, BY, CREATE, PROCEDURE, FUNCTION, TRIGGER, BEGIN, END, DECLARE, IF, ELSE, RETURN, SET, INSERT, UPDATE, DELETE, INTO, VALUES, SIGNAL, JOIN, ON, AND, IN, GROUP, AS, RETURNS, DETERMINISTIC, COUNT, IFNULL, SQL, DATA, READS, THEN, AFTER, FOR, EACH, ROW, OLD, NEW, ROUND, CONSTRAINT, FOREIGN, KEY, REFERENCES, PRIMARY, CHECK, TABLE, DEFAULT, DESC, UNION, ALL, OVER, WHILE, DO, LEAVE, EXISTS, BEFORE, OR, IDENTIFIED, GRANT, TO, LEFT, RIGHT},
  keywordstyle=\color{blue}\bfseries,
  morekeywords=[2]{INT, CHAR, VARCHAR, BOOLEAN, DATE, DECIMAL, ENUM, TIME, CASCADE, NULL, NOT, USER, PRIVILEGES, TRUE},
  keywordstyle=[2]\color{red}\bfseries,
  comment=[l]{--},
  commentstyle=\color{gray}\ttfamily,
  morestring=[b]',
  stringstyle=\color{teal},
  basicstyle=\ttfamily\small,
  showstringspaces=false,
  breaklines=true
}

\lstset
{
  language=SQL,
  frame=single,
  breaklines=true,
  columns=flexible,
  keepspaces=true,
  numbers=left,
  numberstyle=\tiny\color{gray},
  backgroundcolor=\color{white}
}
\newcommand{\sql}[1]
{
  \begin{lstlisting}[language=sql]
    #1
  \end{lstlisting}
}
\newcommand{\python}[1]
{
  \begin{lstlisting}[language=python]
    #1
  \end{lstlisting}
}
\AtBeginDocument{\counterwithin{lstlisting}{section}}

\begin{document}
\fancyfoot{}
\coverpage\clearpage

\tableofcontents\clearpage
\listoffigures\clearpage
\setcounter{page}{1}
\fancyfoot[L]{\scriptsize \ttfamily Data Mining -- Assignment Report\\1$^{\texttt{st}}$ Semester -- Academic year 2025-2026}
\fancyfoot[R]{\scriptsize \ttfamily Page {\thepage}/\pageref{LastPage}}
\section{Introduction and Project Objectives}
\subsection{Aim of the Project}
\subsection{Specific Objectives}
\clearpage
\section{Exploratory Data Analysis}
\subsection{Data Loading and Initial Inspection}
The dataset was loaded from a CSV file (\texttt{StressLevelDataset.csv}), which was obtained from \href{https://www.kaggle.com/datasets/mdsultanulislamovi/student-stress-monitoring-datasets}{\textcolor{blue}{\textit{Kaggle}}} and imported into a Pandas DataFrame for analysis. This dataset offers a comprehensive overview of factors that influence student stress, spanning psychological, physiological, environmental, academic, and social aspects. It is suitable for tasks such as stress prediction and factor analysis, with all data provided in numerical format for straightforward processing.
\begin{itemize}
  \item \textbf{Data Shape:} The dataset consists of \texttt{1100} rows (instances) and \texttt{21} columns (\texttt{20} features + \texttt{1} target variable). This size is sufficient for exploratory analysis and modeling without overwhelming computational resources.
  \item \textbf{Data Quality:} There are no missing values or duplicate rows, indicating a clean and well-prepared dataset. All columns are of type \texttt{int64}, confirming numerical data with no need for type conversion or imputation. No categorical strings or timestamps are present, simplifying preprocessing.
  \item \textbf{Column Names and Descriptions:} The features cover a range of student experiences, measured on ordinal scales. Most are rated from \texttt{0} (low/absent) to \texttt{5} (high/severe), except for a few with wider rangegs based on standard psychological metrics.
  \item \textbf{Target Variable Distribution:} The target variable (\texttt{stress\_level}) is a multiclass label and is balanced among the classes. This near-equal distribution reduces bias in classification models and reflects a representative sample across stress severities.\\
        \begin{minipage}{0.35\textwidth}
          \centering
          \includegraphics[width=\textwidth]{../images/stress_level_distribution.png}
          \label{stress_level_distribution}
        \end{minipage}
        \hfill
        \begin{minipage}{0.65\textwidth}
          \begin{itemize}
            \item Level 0 (low stress): \texttt{373} instances ($\sim$33.9\%)
            \item Level 1 (medium stress): \texttt{358} instances ($\sim$32.5\%)
            \item Level 2 (high stress): \texttt{369} instances ($\sim$33.6\%)
          \end{itemize}
        \end{minipage}
\end{itemize}

\subsubsection*{Statistical Insights}
A summary of some basic statistics for numerical features is provided below (computed via \texttt{.describe()} method), shows reasonable variability across features, with no extreme outliers apparent at first glance.
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../images/describe.png}
  \caption{Some Summary Statistics}
  \label{describe}
\end{figure}
\begin{enumerate}
  \item \textbf{Psychological Factors:}
        \begin{itemize}
          \item \texttt{anxiety\_level} and \texttt{depression} have high variability, indicating diverse mental health states among students.
          \item \texttt{self\_esteem} is generally positive but with noticeable spread.
          \item \texttt{mental\_health\_history} is nearly evenly split, useful as a binary predictor.
        \end{itemize}
  \item \textbf{Physiological Factors:}
        \begin{itemize}
          \item Features like \texttt{headache}, \texttt{breathing\_problem}, and \texttt{sleep\_quality} mostly cluster around mid-values; notably, a subset reports poor sleep.
          \item \texttt{blood\_pressure} is more often normal or high.
        \end{itemize}
  \item \textbf{Environmental/Social Factors:}
        \begin{itemize}
          \item Most students report moderate levels in aspects like \texttt{noise\_level}, \texttt{living\_conditions}, and \texttt{safety}, but outliers exist.
          \item \texttt{social\_support} is moderate, with some reporting low support.
          \item Stressors such as \texttt{bullying} and \texttt{peer\_pressure} are mid-range on average but right-skewed.
        \end{itemize}
  \item \textbf{Academic Factors:} Academic features (\texttt{academic\_performance}, \texttt{study\_load}, etc.) are generally balanced, with means close to the scale midpoint.
  \item \textbf{Target Variable (\texttt{stress\_level}):} Almost perfectly balanced across all levels, making the dataset suitable for classification tasks.
\end{enumerate}

\paragraph*{Overall Insights:}
\begin{itemize}
  \item The dataset is balanced and clean, with no missing values (all feature counts = 1100).
  \item Psychological metrics such as {\tt anxiety\_level} and {\tt depression} exhibit higher variability (greater standard deviation relative to their range) compared to ordinal 0--5 features, which cluster around std~$1.4$. This suggests more diverse experiences in mental health aspects among students than in environmental or academic features.
  \item Quartile analysis shows that many stressor-related features (e.g., {\tt bullying}) are slightly right-skewed: the lower quartile (25\%) is low, while the upper quartile (75\%) is considerably higher. Thus, while most students report moderate conditions, a minority experience much higher levels of specific stressors.
  \item Features with greater variance (like anxiety and depression) are expected to be stronger predictors of stress, whereas environmental variables, despite moderate means, may serve more as moderators.
  \item Minimum and maximum values largely stay within the designated feature ranges, with no obvious extreme outliers. These distributions will be further examined using violin plots later in the report to visually confirm the absence of anomalies and illustrate how feature spreads vary by stress level.
\end{itemize}

\subsection{Data Visualization and Insights}
Visualizations were generated using \texttt{Matplotlib} and \texttt{Seaborn} to explore distributions, relationships, and patterns. All figures were saved to the images directory for inclusion in the report. Key visualizations and insights are described below.
\subsubsection{Correlation Heatmap}
This visualization highlights key relationships influencing student stress, revealing distinct patterns in psychological, physiological, environmental, academic, and social factors. Notably, the correlations are generally moderate to strong ($|r| > 0.6$ for many pairs), indicating interconnected dimensions of student well-being.

\noindent
\begin{figure}[H]
  \centering
  \includegraphics[width=1.1\textwidth]{../images/correlation_heatmap.png}
  \caption{Correlation Heatmap}
  \label{corr_heatmap}
\end{figure}

\paragraph*{Correlation Analysis Insights:}
\begin{enumerate}
  \item \textbf{Strongest Positive Correlations with Stress Level:}
        \begin{itemize}
          \item \textbf{Primary stress amplifiers:} The highest include \texttt{bullying} ($r=0.75$), \texttt{anxiety\_level} ($r=0.74$), \texttt{future\_career\_concerns} ($r=0.74$), \texttt{depression} ($r=0.73$), \texttt{headache} ($r=0.71$), and \texttt{peer\_pressure} ($r=0.71$). These align with research showing that bullying and academic pressures contribute to elevated stress and related mental health issues, such as self-harm behaviors in middle school students. Similarly, anxiety and depression are well-documented predictors of stress, often forming a vicious cycle that impacts academic outcomes.
          \item \textbf{Secondary contributors:} \texttt{noise\_level} ($r=0.68$), \texttt{extracurricular\_activities} ($r=0.67$), \texttt{study\_load} ($r=0.65$), and \texttt{mental\_health\_history} ($r=0.65$), implying that environmental noise and overloaded schedules exacerbate stress, consistent with studies on chronic academic stress leading to insufficient sleep and negative affect.
          \item \textbf{Physiological indicators:} \texttt{blood\_pressure} ($r=0.39$) and \texttt{breathing\_problem} ($r=0.37$) show milder associations, potentially indicating secondary stress manifestations.
        \end{itemize}

  \item \textbf{Strongest Negative Correlations with Stress Level:}
        \begin{itemize}
          \item \textbf{Key protective factors:} \texttt{self\_esteem} ($r=-0.76$), \texttt{sleep\_quality} ($r=-0.75$), \texttt{basic\_needs} ($r=-0.72$), \texttt{living\_conditions} ($r=-0.71$), \texttt{safety} ($r=-0.71$), and \texttt{academic\_performance} ($r=-0.71$). This suggests that fulfilling basic needs, safe environments, and strong academic achievement act as buffers against stress, corroborating findings that self-esteem and sleep quality are crucial predictors of lower stress levels among students.
          \item \textbf{Social support factors:} \texttt{social\_support} ($r=-0.68$) and \texttt{teacher\_student\_relationship} ($r=-0.63$). This highlights the role of supportive networks in reducing stress, as supported by research on mental health in first-year university students.
        \end{itemize}

  \item \textbf{Inter-Correlation Clusters:}
        \begin{itemize}
          \item \textbf{Mental health vulnerability cluster:} \texttt{anxiety\_level} with \texttt{depression} ($r=0.69$), both strongly linked with \texttt{mental\_health\_history} ($r \approx 0.63{-}0.70$), indicating these factors often co-occur and amplify stress.
          \item \textbf{Resilience cluster:} \texttt{self\_esteem}, \texttt{sleep\_quality}, \texttt{academic\_performance}, and \texttt{social\_support} (inter-correlations $r \approx 0.50{-}0.70$), all negatively associated with stress-related variables.
          \item \textbf{Social pressure cluster:} \texttt{bullying}, \texttt{peer\_pressure}, and \texttt{future\_career\_concerns} (inter-correlations $r \approx 0.50{-}0.65$), reinforcing their collective impact on stress levels.
        \end{itemize}

  \item \textbf{Implications for Modeling and Multicollinearity:}
        \begin{itemize}
          \item \textbf{Multicollinearity concerns:} High inter-correlations (\texttt{depression} and \texttt{self\_esteem} at $r=-0.70$, \texttt{anxiety\_level} and \texttt{sleep\_quality} at $r=-0.66$) signal potential multicollinearity that could inflate variance in regression models.
          \item \textbf{Recommended techniques:} Variance inflation factor (VIF) checks or principal component analysis (PCA) are recommended to address multicollinearity issues.
          \item \textbf{Feature selection priority:} The heatmap provides a foundation for feature selection in predictive modeling, prioritizing high-correlation variables like \texttt{bullying} and \texttt{self\_esteem}.
          \item \textbf{Intervention targets:} Patterns suggest that interventions targeting mental health (reducing anxiety and improving sleep) and social factors (anti-bullying programs) could effectively lower stress levels.
        \end{itemize}
\end{enumerate}
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../images/high_corr_scatterplots.png}
  \caption{Scatterplots of Highly Correlated Pairs $|r| \geq 0.7$}
  \label{highly_corr_pairs}
\end{figure}
\paragraph*{Insights:}
\begin{itemize}
  \item The scatterplots highlight pairs with $|r| \geq 0.7$ from the correlation analysis: Each pair exhibits a strong positive linear trend with dense clustering and few outliers, echoing the correlation heatmap and underscoring the interconnectedness of key student stressors.
  \item In data mining, these high correlations signal potential multicollinearity, making variance inflation factor (VIF) assessment or principal component analysis (PCA) necessary for dimensionality reduction to improve model stability in stress prediction.
  \item The pronounced linear relationships support the use of regression-based predictive models, while the presence of tight point clusters suggests the applicability of clustering algorithms to identify subgroups and enable targeted feature engineering.
\end{itemize}

\subsubsection{Key Features Distributions}
The following visualizations are essential in data mining for identifying patterns, skewness, and potential preprocessing needs, such as normalization or transformation, to enhance model performance in classification tasks like this topic.
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../images/numeric_histograms.png}
  \caption{Key Features Distributions}
  \label{key_hists}
\end{figure}
\paragraph*{Insights:}
\begin{enumerate}
  \item \textbf{Symmetric and Bell-Shaped Distributions}
        \begin{itemize}
          \item Psychological features like \texttt{anxiety\_level} and \texttt{depression} exhibit approximately normal distributions with peaks in the moderate range.
          \item Indicates widespread moderate symptoms among students, aligning with research showing prevalent anxiety and depression linked to academic pressures.
          \item For data mining, such normal distributions are ideal for parametric models (e.g., linear regression) with minimal need for transformation.
          \item High variability ($\mathrm{std}\approx 6$--$7$) suggests feature scaling may be necessary for algorithms such as SVM or neural networks, since these models are sensitive to differences in feature scales and may perform poorly or converge slowly if features are not normalized.
        \end{itemize}

  \item \textbf{Binary and Imbalanced Distribution}
        \begin{itemize}
          \item \texttt{Mental\_health\_history} is binary and slightly imbalanced ($\sim$55\% at 0, mean 0.49, median 0.00): roughly half of students report no prior mental health issues.
          \item This feature could serve as a strong categorical predictor in mining tasks but may require stratified sampling to avoid bias.
          \item Its skewed distribution (mode at 0) highlights possible class imbalance, suggesting oversampling techniques may be needed.
        \end{itemize}

  \item \textbf{Skewed Distributions in Social and Academic Stressors}
        \begin{itemize}
          \item Features like \texttt{future\_career\_concerns}, \texttt{peer\_pressure}, and \texttt{bullying} show positive skewness (right-tailed) with modes at lower values.
          \item Most students experience low-to-moderate levels, but some face intense issues, consistent with research on adolescent stress.
          \item For data mining, applying log transformations or binning may normalize these features or optimize them for tree-based models.
        \end{itemize}

  \item \textbf{Multimodal or Mildly Skewed Distributions}
        \begin{itemize}
          \item \texttt{Headache} and \texttt{extracurricular\_activities} have bimodal patterns; \texttt{noise\_level} is more symmetric with a central mode at $3$.
          \item These indicate clusters in student experiences -- such as differing levels of environmental noise or activity participation -- which may impact stress.
          \item In data mining, multimodal features suggest exploring clustering algorithms (e.g., K-means) to segment subgroups, since multiple peaks often indicate the presence of distinct groups within the data.
          \item Mean-median discrepancies indicate mild left skew; outlier checks and robust scaling may be warranted.
        \end{itemize}

  \item \textbf{Data Mining Implications}
        \begin{itemize}
          \item The dataset exhibits a mix of normal and skewed features; this informs preprocessing strategies such as feature scaling and strategies for skewness.
          \item Agreement between mean and median in symmetric features supports using standard central tendency; skewed features highlight opportunities for feature engineering.
          \item These findings guide further analyses, such as bivariate plots against \texttt{stress\_level}, to investigate conditional distributions for predictive modeling.
        \end{itemize}
\end{enumerate}

\subsubsection{Detecting Anomalies and Outliers}
The violin plots provide a bivariate view of the distributions, combining density estimates with boxplot elements to reveal spreads, modes, and potential outliers. These visualizations are crucial in data mining for detecting anomalies, assessing group differences, and informing preprocessing steps like outlier handling, which can significantly impact model accuracy in classification or regression tasks. The plots highlight how feature values shift across stress categories, with wider violins indicating higher variability and tails signaling outliers, often more pronounced in high-stress groups.
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../images/violinplots_by_stress_level.png}
  \caption{Violin Plots}
  \label{violinplots}
\end{figure}

\paragraph*{Insights:}
\begin{enumerate}
  \item \textbf{Increasing Trends with Stress Levels}
        \begin{itemize}
          \item Psychological and social features including \texttt{anxiety\_level}, \texttt{depression}, \texttt{bullying}, \texttt{peer\_pressure}, and \texttt{future\_career\_concerns} exhibit clear positive shifts across stress categories: low-stress groups present with narrow, low-value densities, while high-stress demonstrate broader, higher-value distributions.
          \item Features such as \texttt{headache} and \texttt{noise\_level} also rise notably, with medians progressing from $\sim$1--2 (low stress) to $\sim$3--4 (high stress).
          \item This trend is consistent with educational data mining findings, where engineered features -- such as binned or transformed high tails -- improve model interpretability of stress predictors.
          \item \texttt{Extracurricular\_activities} trends upward as well, implicating overload as a contributing factor, in line with studies on academic performance stressors.
        \end{itemize}

  \item \textbf{Binary Feature Insights}
        \begin{itemize}
          \item The binary feature \texttt{Mental\_health\_history} manifests as thin spikes: level 0 is concentrated at $0$, whereas level 2 becomes bimodal with notable density at $1$.
          \item This suggests a threshold effect valuable for decision rules in tree-based models, where binary splits on such features effectively classify stress levels.
        \end{itemize}

  \item \textbf{Outlier Detection and Variability}
        \begin{itemize}
          \item Extended tails in high-stress violin plots -- notably for \texttt{depression} and \texttt{anxiety} (outliers beyond 20--25 at level 2), and \texttt{bullying} or \texttt{peer\_pressure} (up to $5$) -- highlight potential anomalies.
          \item The increase in feature variability (wider violins at level 2) signals heteroscedasticity, justifying the use of robust preprocessing to facilitate better generalization.
        \end{itemize}

  \item \textbf{Data Mining Implications}
        \begin{itemize}
          \item These observations emphasize the need for outlier-aware preprocessing in student stress prediction pipelines, as anomalous values (especially in \texttt{depression}) can unduly influence feature importance metrics.
          \item Clustering techniques may help segment and analyze outlier subpopulations, enhancing model robustness.
          \item Collectively, these insights inform feature selection and transformation choices, ultimately supporting improved predictive performance for multiclass stress classification -- mirroring findings in related data mining competitions and research.
        \end{itemize}
\end{enumerate}

\section{Preprocessing Pipeline}
\subsubsection*{Anomaly Detection and Data Cleaning}
Althouugh the dataset is reported to contain no missing values (NaN), we nonetheless perform through data cleaning as follow to ensure integrity and consistency:
\begin{lstlisting}[language=python]
# Handle missing values
missing_values = df.isnull().sum()
print(f"Missing values per column:\n{missing_values}")
numeric_cols = self.data.select_dtypes(include=[np.number]).columns
categorical_cols = data.select_dtypes(include=['object']).columns
# Impute numeric columns
if len(numeric_cols) > 0:
    imputer = SimpleImputer(strategy=strategy)
    data[numeric_cols] = imputer.fit_transform(data[numeric_cols])
# Impute categorical columns
if len(categorical_cols) > 0:
    imputer = SimpleImputer(strategy='most_frequent')
    data[categorical_cols] = imputer.fit_transform(data[categorical_cols])
# Handle duplicates
duplicate_count = df.duplicated().sum()
print(f"Number of duplicate rows: {duplicate_count}")
data = data.drop_duplicates()
\end{lstlisting}
\subsubsection*{Anomaly Cleaning and Applying SMOTE }
\begin{lstlisting}[language=python]
for col in columns:
  Q1 = df[col].quantile(0.25)
  Q3 = df[col].quantile(0.75)
  IQR = Q3 - Q1
  lower_bound = Q1 - 1.5 * IQR
  upper_bound = Q3 + 1.5 * IQR
  df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
# SMOTE
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X, y)
\end{lstlisting}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../images/violinplots_iqr_cleaned.png}
  \caption{Anomalies Cleaned violinplots}
  \label{cleanedviolin}
\end{figure}
\subsubsection*{Encode Categorical Variables}
\begin{lstlisting}[language=python]
from sklearn.preprocessing import LabelEncoder
for col in columns:
  if col in data.columns:
      data[col] = label_encoder.fit_transform(data[col].astype(str))
\end{lstlisting}
\subsubsection*{Split Intro Train \& Validating Data}
\begin{lstlisting}[language=python]
X = data.drop(columns=[target_col])
y = data[target_col]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=test_size, random_state=42, stratify=y
)
\end{lstlisting}
\section{Training Pipeline}
\subsection{Models Selection}

\subsubsection{Classifier Models}
The dataset analyzed comprises $1100$ instances and $20$ features, encompassing numerical, categorical, and discrete integer variables, designed to classify students as dropout, enrolled, or graduate. While no missing values were present, the dataset exhibited significant class imbalance, providing a realistic scenario for educational predictive modeling focused on the early identification of at-risk stressed students using machine learning classifiers.
\begin{lstlisting}[language=python]
models = {
'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),
'SVM': SVC(kernel='rbf', random_state=42),
'KNN': KNeighborsClassifier(n_neighbors=5),
'Naive Bayes': GaussianNB(),
'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
}
\end{lstlisting}
Seven classification model pipelines were evaluated in total, but this report emphasizes tree-based ensemble methods -- Random Forest and Gradient Boosting -- selected for their aptitude in managing mixed feature types, interpretability, and robust performance under imbalanced conditions.
\subsubsection*{Random Forest Classifier}
Random Forest works by creating an ensemble of multiple decision trees, each trained on a random subset of the data (bootstrapping) and features, then aggregating their predictions through majority voting for classification or averaging for regression. This reduces overfitting, handles variance well, and improves accuracy by leveraging diversity among the trees.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{../images/rf.png}
  \caption{Random Forest Classifier}
  \label{rf_clf}
\end{figure}
Random Forest (RF) is suitable for this dataset as an ensemble method that builds multiple decision trees via bagging, effectively handling mixed feature types, non-linear relationships, and class imbalance with techniques like class weighting. It reduces overfitting through randomness and provides feature importance scores, offering interpretable insights into factors like socio-economic status or grades that influence dropout, while achieving robust performance on tabular data without extensive preprocessing.

\subsubsection*{Gradient Boosting Classifier}
Gradient Boosting builds trees sequentially, where each new tree corrects the errors of the previous ones by minimizing a loss function using gradient descent. It focuses on hard-to-predict instances, often achieving high performance through additive modeling, with regularization to prevent overfitting.
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../images/gb.png}
  \caption{Gradient Boosting Classifier}
  \label{gb_clf}
\end{figure}
Gradient Boosting excels by sequentially building trees to correct errors, capturing complex interactions and subtle patterns in the data better than single models. It addresses imbalance with built-in weighting and regularization to prevent overfitting, often yielding higher accuracy on imbalanced educational datasets, and includes feature importance for explaining predictions, making it a strong choice for model selection to optimize metrics like F1-score in student success forecasting.

\subsubsection*{Clustering Models}
Unsupervised clustering models were applied to discover latent groupings among students, offering insights that complement supervised classification. Principal Component Analysis (PCA) was used beforehand to reduce dimensionality while retaining most variance, enabling effective visualization and interpretation of clusters.
\begin{lstlisting}[language=python]
  from sklearn.decomposition import PCA
  from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
  # Apply PCA for reduction
  X_pca = PCA(n_components=2, random_state=42).fit_transform(X_scaled)
  # K-Means
  kmeans = KMeans(n_clusters=3, random_state=42)
  labels_kmeans = kmeans.fit_predict(X_pca)
  # Hierarchical
  hier = AgglomerativeClustering(n_clusters=3)
  labels_hier = hier.fit_predict(X_pca)
  # DBSCAN
  dbscan = DBSCAN(eps=0.5, min_samples=5)
  labels_dbscan = dbscan.fit_predict(X_pca)
  \end{lstlisting}

Clustering revealed distinct student groups based on multivariate patterns underlying stress and academic engagement. K-Means and hierarchical clustering provided grouping stability and interpretability, while DBSCAN offered robust detection of anomalies and noise -- crucial for flagging at-risk students who may otherwise be masked in class-centric analyses. Visualizations and silhouette scores guided the evaluation and naming of meaningful clusters used for downstream profiling and recommendations.
\subsubsection*{K-Means Clustering}
K-Means is a partition-based clustering algorithm that groups data into $k$ non-overlapping clusters by minimizing the within-cluster sum-of-squares. Each data point is assigned to the nearest cluster centroid, and centroids are iteratively updated until assignments stabilize. This method is well-suited for datasets with quantitative features and helps to reveal underlying group structures by forming compact, clearly-separated clusters.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../images/kmeans.png}
  \caption{K-Means Clustering}
  \label{kmeans}
\end{figure}

For this dataset, K-Means performs robustly due to its completely numerical feature set, balanced class distribution, and absence of missing values. The consistent scaling and moderate size of the data allow for fast convergence and reliable results. These factors enable K-Means to effectively identify groups of students who share similar stress profiles and academic patterns, supporting the discovery of actionable segments for targeted interventions and recommendations.

\subsubsection*{Hierarchical Clustering}
Hierarchical clustering, specifically agglomerative clustering, builds a nested tree (dendrogram) of clusters by successively merging the two closest data points or clusters until all points are united into a single cluster. This process uses linkage criteria, such as Ward's method, to define cluster distances and can reveal rich structural relationships within the dataset at multiple granularities.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../images/hierarchical.png}
  \caption{Hierarchical (Agglomerative) Clustering and Dendrogram}
  \label{hierarchical}
\end{figure}

Hierarchical clustering is particularly valuable for this student dataset, offering interpretable visual groupings through dendrograms that expose both global and local patterns among students' stress and academic profiles. Because our features are purely numerical and well-scaled, hierarchical clustering produces meaningful splits without being skewed by differing ranges or noise. The ability to visualize the merge steps assists in selecting the most natural number of clusters, while also detecting subgroups and anomalous cases. This complements K-Means by clarifying nested relationships, supporting granular targeting in interventions and profiling strategies.

\subsubsection*{DBSCAN Clustering}
DBSCAN (Density-Based Spatial Clustering of Applications with Noise) forms clusters by connecting points close together in dense regions and labels points in sparse areas as noise (outliers). Unlike K-Means or hierarchical clustering, DBSCAN does not require the number of clusters to be specified in advance, and it can discover clusters of irregular shape and variable size.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{../images/dbscan.png}
  \caption{DBSCAN Clustering}
  \label{dbscan}
\end{figure}

DBSCAN is particularly powerful in this student context for detecting minority subgroups or atypical students whose stress or engagement profiles are substantially different from their peers. By identifying outliers, DBSCAN helps flag at-risk individuals who might not belong to any major cluster and could otherwise be overlooked by centroid- or linkage-based methods. The ability to tune \texttt{eps} (radius threshold) and \texttt{min\_samples} provides flexibility for handling varying densities in student survey responses, revealing both tight-knit groups and peripheral cases critical for intervention.
\subsection{Advanced Techniques}
\subsubsection*{Principal Component Analysis (PCA)}
Principal Component Analysis (PCA) is an unsupervised technique for dimensionality reduction that transforms correlated original features into a smaller number of uncorrelated variables, called principal components, capturing the maximum variance present in the data.

\noindent\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../images/pca.png}
  \caption{PCA algorithm}
  \label{pca}
\end{figure}
PCA is well-suited for our student stress dataset (1100 samples, 20 numerical features), which shows strong inter-feature correlations (e.g., \texttt{anxiety\_level} and \texttt{depression} $r=0.69$). This multicollinearity can inflate model variance and cause overfitting. PCA solves this by projecting data onto orthogonal axes of maximum variance. With standardized numeric features, PCA efficiently reduces dimensions while retaining at least $95\%$ of the variance.

For this data, PCA typically reduces dimensionality to $8$-$10$ principal components, with the main components reflecting variability in psychological and social factors. Higher-variance features dominate early components, improving interpretability and classifier performance.

\noindent
\begin{lstlisting}[language=python]
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
# Standardize features before PCA
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
# Fit PCA to retain 95% of explained variance
pca = PCA(n_components=0.95, random_state=42)
X_pca = pca.fit_transform(X_scaled)
\end{lstlisting}

PCA thus mitigates the curse of dimensionality, addresses multicollinearity, and reduces noise, benefiting downstream supervised and unsupervised learning tasks.

\subsection{Recursive Feature Elimination (RFE)}
Recursive Feature Elimination (RFE) is a supervised feature selection method that recursively removes the least important features according to a model's coefficients or feature importances, ranking all features and selecting the top subset.

\noindent\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../images/rfe.png}
  \caption{RFE algorithm}
  \label{rfe}
\end{figure}
In this project, RFE is well-suited because our 20 input features vary in predictive strength: features like \texttt{anxiety\_level}, \texttt{depression}, \texttt{bullying}, and \texttt{future\_career\_concerns} show strong correlation with stress, while others contribute less. We use a Random Forest estimator in RFE -- ideal for our numerical, mildly imbalanced data and its non-linear feature interactions. Our dataset's high quality (no missing or duplicate records, balanced targets) supports reliable feature ranking. RFE works particularly well after PCA by removing residual redundancy, helping reduce dimensionality while retaining essential predictors. For this analysis, we selected 10 features to optimize simplicity and prevent overfitting.

\noindent
\begin{lstlisting}[language=python]
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE
# RFE setup with Random Forest (selecting 10 key features)
estimator = RandomForestClassifier(random_state=42)
rfe = RFE(estimator=estimator, n_features_to_select=10)
X_rfe = rfe.fit_transform(X, y)
selected_features = X.columns[rfe.support_]
\end{lstlisting}

RFE efficiently narrows down the feature set, enhancing both classification and clustering by concentrating on the most informative predictors. In our analysis, RFE selected features that correspond with inter-correlation clusters in the data (e.g., academic factors such as \texttt{study\_load}), frequently retaining the strongest variable from each domain -- such as \texttt{bullying} from social factors and \texttt{anxiety\_level} from psychological factors. This targeted selection increases the predictive power of models, as demonstrated in prior educational data mining research where using RFE-selected features led to improved classification accuracy for student stress prediction.
\section{Results and Analysis}

\subsection{Classification Results}

\subsection{Clustering Results}

\subsection{Model Comparison}







































\end{document}