\documentclass[twoside,final]{hcmut-report}

\usepackage[utf8]{inputenc}
\usepackage[T5]{fontenc}
\usepackage[protrusion=false]{microtype}

\usepackage{graphicx, caption}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{multirow,multicol}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{float}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{tabularx}
\usepackage{makecell}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{tcolorbox}
\usepackage{lastpage}

\definecolor{keywordcolor}{rgb}{0.13,0.13,1} % Blue keywords
\definecolor{stringcolor}{rgb}{0.7,0.13,0.13} % Dark red strings
\definecolor{commentcolor}{rgb}{0.25,0.5,0.35} % Green comments
\definecolor{backgroundcolor}{rgb}{0.97,0.97,0.97} % Light background
\definecolor{numbercolor}{rgb}{0.85,0.65,0.13} % Numbers

\lstdefinelanguage{Python}{
  morekeywords={
    and, as, assert, async, await, break, class, continue, def, del, elif, else, except,
    False, finally, for, from, global, if, import, in, is, lambda, None, nonlocal, not, 
    or, pass, raise, return, True, try, while, with, yield
  },
  keywordstyle=\color{keywordcolor}\bfseries,
  morekeywords=[2]{bool, bytes, bytearray, complex, dict, float, frozenset, int, list, object, set, str, tuple},
  keywordstyle=[2]\color{blue}\bfseries,
  identifierstyle=\color{black},
  sensitive=true,
  comment=[l]{\#},
  morecomment=[s]{'''}{'''}, 
  morecomment=[s]{"""}{"""},
  commentstyle=\color{commentcolor}\ttfamily,
  stringstyle=\color{stringcolor}\ttfamily,
  morestring=[b]',
  morestring=[b]",
  % Highlight numbers
  literate=
    *{0}{{{\color{numbercolor}0}}}1
    {1}{{{\color{numbercolor}1}}}1
    {2}{{{\color{numbercolor}2}}}1
    {3}{{{\color{numbercolor}3}}}1
    {4}{{{\color{numbercolor}4}}}1
    {5}{{{\color{numbercolor}5}}}1
    {6}{{{\color{numbercolor}6}}}1
    {7}{{{\color{numbercolor}7}}}1
    {8}{{{\color{numbercolor}8}}}1
    {9}{{{\color{numbercolor}9}}}1
    {.0}{{{\color{numbercolor}.0}}}2
    {.1}{{{\color{numbercolor}.1}}}2
    {.2}{{{\color{numbercolor}.2}}}2
    {.3}{{{\color{numbercolor}.3}}}2
    {.4}{{{\color{numbercolor}.4}}}2
    {.5}{{{\color{numbercolor}.5}}}2
    {.6}{{{\color{numbercolor}.6}}}2
    {.7}{{{\color{numbercolor}.7}}}2
    {.8}{{{\color{numbercolor}.8}}}2
    {.9}{{{\color{numbercolor}.9}}}2
    {e+}{{{\color{numbercolor}e+}}}2
    {e-}{{{\color{numbercolor}e-}}}2
    {\#\#NUMSTART\#\#}{}{0}
    {\#\#NUMEND\#\#}{}{0},
}
\lstset{
  language=Python,
  backgroundcolor=\color{backgroundcolor},
  basicstyle=\ttfamily\small,
  numbers=left,
  numberstyle=\tiny\color{gray},
  stepnumber=1,
  numbersep=10pt,
  tabsize=2,
  showspaces=false,
  showstringspaces=false,
  breaklines=true,
  breakatwhitespace=true,
  frame=single,
}

\lstdefinelanguage{TypeScript}{
  keywords={abstract, as, boolean, break, case, catch, class, continue, const, constructor, debugger, declare, default, delete, do, else, enum, export, extends, false, finally, for, from, function, get, if, implements, import, in, infer, instanceof, interface, is, keyof, let, module, namespace, never, new, null, number, object, of, package, private, protected, public, readonly, require, global, return, set, static, string, super, switch, symbol, this, throw, true, try, type, typeof, undefined, unique, unknown, var, void, while, with, yield, async, await},
  keywordstyle=\color{keywordcolor}\bfseries,
  ndkeywords={string, number, boolean, Promise, any, void},
  ndkeywordstyle=\color{blue}\bfseries,
  identifierstyle=\color{black},
  sensitive=true,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{commentcolor}\ttfamily,
  stringstyle=\color{stringcolor}\ttfamily,
  morestring=[b]',
  morestring=[b]",
}

\lstset{
  language=TypeScript,
  backgroundcolor=\color{backgroundcolor},
  basicstyle=\ttfamily\small,
  numbers=left,
  numberstyle=\tiny\color{gray},
  stepnumber=1,
  numbersep=10pt,
  tabsize=2,
  showspaces=false,
  showstringspaces=false,
  breaklines=true,
  breakatwhitespace=true,
  frame=single,
}
\lstdefinelanguage{EnvCustom}{
  keywords={VITE_APP_API_URL, DATABASE_URL, DB_HOST, DB_USER, DB_PASSWORD, DB_PORT, DB_NAME, JWT_SECRET},
  keywordstyle=\color{blue}\bfseries,
  sensitive=true,
  comment=[l]{\#},
  commentstyle=\color{gray}\ttfamily,
  morestring=[b]',
  morestring=[b]",
  stringstyle=\color{red}\ttfamily,
  identifierstyle=\color{black}
}

\lstdefinelanguage{JavaScriptCustom}{
  keywords={
    break, case, catch, class, const, continue, debugger, default, delete, do, else, export,
    extends, finally, for, function, if, import, in, instanceof, let, new, return, super, 
    switch, this, throw, try, typeof, var, void, while, with, yield, await, async,
    mysql, dotenv, process, config, createPool, getConnection, release
  },
  keywordstyle=\color{blue}\bfseries,
  ndkeywords={true, false, null, undefined, NaN, Infinity},
  ndkeywordstyle=\color{teal}\bfseries,
  identifierstyle=\color{black},
  sensitive=true,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{gray}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]"
}


\lstdefinelanguage{SQL}
{
  keywords={SELECT, FROM, WHERE, GROUP BY, HAVING, ORDER, BY, CREATE, PROCEDURE, FUNCTION, TRIGGER, BEGIN, END, DECLARE, IF, ELSE, RETURN, SET, INSERT, UPDATE, DELETE, INTO, VALUES, SIGNAL, JOIN, ON, AND, IN, GROUP, AS, RETURNS, DETERMINISTIC, COUNT, IFNULL, SQL, DATA, READS, THEN, AFTER, FOR, EACH, ROW, OLD, NEW, ROUND, CONSTRAINT, FOREIGN, KEY, REFERENCES, PRIMARY, CHECK, TABLE, DEFAULT, DESC, UNION, ALL, OVER, WHILE, DO, LEAVE, EXISTS, BEFORE, OR, IDENTIFIED, GRANT, TO, LEFT, RIGHT},
  keywordstyle=\color{blue}\bfseries,
  morekeywords=[2]{INT, CHAR, VARCHAR, BOOLEAN, DATE, DECIMAL, ENUM, TIME, CASCADE, NULL, NOT, USER, PRIVILEGES, TRUE},
  keywordstyle=[2]\color{red}\bfseries,
  comment=[l]{--},
  commentstyle=\color{gray}\ttfamily,
  morestring=[b]',
  stringstyle=\color{teal},
  basicstyle=\ttfamily\small,
  showstringspaces=false,
  breaklines=true
}

\lstset
{
  language=SQL,
  frame=single,
  breaklines=true,
  columns=flexible,
  keepspaces=true,
  numbers=left,
  numberstyle=\tiny\color{gray},
  backgroundcolor=\color{white}
}
\newcommand{\sql}[1]
{
  \begin{lstlisting}[language=sql]
    #1
  \end{lstlisting}
}
\newcommand{\python}[1]
{
  \begin{lstlisting}[language=python]
    #1
  \end{lstlisting}
}
\AtBeginDocument{\counterwithin{lstlisting}{section}}

\begin{document}
\fancyfoot{}
\coverpage\clearpage

\tableofcontents\clearpage
\listoffigures\clearpage
\setcounter{page}{1}
\fancyfoot[L]{\scriptsize \ttfamily Data Mining -- Assignment Report\\1$^{\texttt{st}}$ Semester -- Academic year 2025-2026}
\fancyfoot[R]{\scriptsize \ttfamily Page {\thepage}/\pageref{LastPage}}
\section*{Abstract}
Academic stress represents a critical challenge in modern university environments, particularly in demanding academic settings such as Vietnam's elite technical institutions. Recent studies reveal that over 70\% of Vietnamese university students experience moderate to high stress levels, endangering both their academic outcomes and personal well-being. The consequences of unmanaged stress in this population are far-reaching: students face increased risks of poor academic performance, sleep disturbances, deteriorating mental health (including anxiety and depression), and, in the most severe scenarios, disengagement and dropout. These patterns align with global research pointing to rising stress among young adults in higher education systems. In this context, the urgency of early detection for at-risk students and the precise identification of the key contributors to academic stress have never been greater. Such proactive interventions form the foundation for effective, targeted student support initiatives.

\section{Introduction}
\subsection{Project Goal}
The primary aim of this project is to develop accurate, robust models that can predict individual student stress levels, while simultaneously extracting and ranking the most influential stress factors from a comprehensive set of inputs. These insights are designed to enable university-based early-warning systems, supporting academic counselors and administrators in the timely identification and assistance of students most in need. Our approach is specifically tailored to the context of Ho Chi Minh City University of Technology (HCMUT), arguably one of Vietnam's most academically rigorous institutions, but our methodology and findings are relevant to a much broader educational landscape.

\subsection{Objectives}
\begin{itemize}
  \item Perform an extensive exploratory data analysis (EDA): Inspect and visualize feature distributions, calculate correlations, detect and understand the nature of outliers, and summarize key patterns that characterize stress within student populations.
  \item Build a comprehensive preprocessing pipeline, including: removal of outliers using the IQR approach, application of Synthetic Minority Over-sampling Technique (SMOTE) to address class imbalance and fairly represent rare stress levels, standardization of features to eliminate scale discrepancies, and use of Principal Component Analysis (PCA) to reduce feature dimensionality while capturing at least 95\% of total variance, thus improving model efficiency and interpretability.
  \item Develop, tune, and systematically compare multiple supervised classification models (including SVM, Random Forests, Logistic Regression, and more) to establish the most reliable approaches for stress prediction, considering both accuracy and practical deployment.
  \item Employ unsupervised clustering methodologies—applied to PCA-reduced data—to discover naturally occurring student groups or “stress phenotypes,” providing fresh insights beyond the constraints of pre-defined labels and enabling data-driven recommendations for subpopulation-specific interventions.
  \item Use tree-based Recursive Feature Elimination (RFE) to perform advanced feature selection: systematically rank all candidate predictors, highlight those most strongly associated with high stress, and provide a concrete basis for policy and intervention design by university stakeholders.
  \item Rigorously evaluate all models and methodologies: Employ comprehensive quantitative metrics -- accuracy, F1-score, confusion matrix, silhouette score for clustering, and k-fold cross-validation -- to measure and compare performance, interpret results in the context of real-world constraints, and clearly present practical recommendations for educators and student support staff.
  \item Discuss broader implications: Link data-driven findings to known psychological, physiological, and social stress theories, and outline how the results can drive policy, health, and academic advising efforts.
\end{itemize}

In summary, this project integrates advanced data mining and machine learning techniques -- from initial data audit through to actionable insights -- to illuminate which factors most powerfully drive academic stress among university students. The methods and conclusions offer both a blueprint for similar educational analytics projects and concrete guidance for Vietnamese university administrators seeking to foster healthier, more resilient student communities.
\clearpage
\section{Exploratory Data Analysis}
\subsection{Data Loading and Initial Inspection}
The dataset was loaded from a CSV file (\texttt{StressLevelDataset.csv}), which was obtained from \href{https://www.kaggle.com/datasets/mdsultanulislamovi/student-stress-monitoring-datasets}{\textcolor{blue}{\textit{Kaggle}}} and imported into a Pandas DataFrame for analysis. This dataset offers a comprehensive overview of factors that influence student stress, spanning psychological, physiological, environmental, academic, and social aspects. It is suitable for tasks such as stress prediction and factor analysis, with all data provided in numerical format for straightforward processing.
\begin{itemize}
  \item \textbf{Data Shape:} The dataset consists of \texttt{1100} rows (instances) and \texttt{21} columns (\texttt{20} features + \texttt{1} target variable). This size is sufficient for exploratory analysis and modeling without overwhelming computational resources.
  \item \textbf{Data Quality:} There are no missing values or duplicate rows, indicating a clean and well-prepared dataset. All columns are of type \texttt{int64}, confirming numerical data with no need for type conversion or imputation. No categorical strings or timestamps are present, simplifying preprocessing.
  \item \textbf{Column Names and Descriptions:} The features cover a range of student experiences, measured on ordinal scales. Most are rated from \texttt{0} (low/absent) to \texttt{5} (high/severe), except for a few with wider rangegs based on standard psychological metrics.
  \item \textbf{Target Variable Distribution:} The target variable (\texttt{stress\_level}) is a multiclass label and is balanced among the classes. This near-equal distribution reduces bias in classification models and reflects a representative sample across stress severities.\\
        \begin{minipage}{0.35\textwidth}
          \centering
          \includegraphics[width=\textwidth]{images/stress_level_distribution.png}
          \label{stress_level_distribution}
        \end{minipage}
        \hfill
        \begin{minipage}{0.65\textwidth}
          \begin{itemize}
            \item Level 0 (low stress): \texttt{373} instances ($\sim$33.9\%)
            \item Level 1 (medium stress): \texttt{358} instances ($\sim$32.5\%)
            \item Level 2 (high stress): \texttt{369} instances ($\sim$33.6\%)
          \end{itemize}
        \end{minipage}
\end{itemize}

\subsubsection*{Statistical Insights}
A summary of some basic statistics for numerical features is provided below (computed via \texttt{.describe()} method), shows reasonable variability across features, with no extreme outliers apparent at first glance.
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{images/describe.png}
  \caption{Some Summary Statistics}
  \label{describe}
\end{figure}
\begin{enumerate}
  \item \textbf{Psychological Factors:}
        \begin{itemize}
          \item \texttt{anxiety\_level} and \texttt{depression} have high variability, indicating diverse mental health states among students.
          \item \texttt{self\_esteem} is generally positive but with noticeable spread.
          \item \texttt{mental\_health\_history} is nearly evenly split, useful as a binary predictor.
        \end{itemize}
  \item \textbf{Physiological Factors:}
        \begin{itemize}
          \item Features like \texttt{headache}, \texttt{breathing\_problem}, and \texttt{sleep\_quality} mostly cluster around mid-values; notably, a subset reports poor sleep.
          \item \texttt{blood\_pressure} is more often normal or high.
        \end{itemize}
  \item \textbf{Environmental/Social Factors:}
        \begin{itemize}
          \item Most students report moderate levels in aspects like \texttt{noise\_level}, \texttt{living\_conditions}, and \texttt{safety}, but outliers exist.
          \item \texttt{social\_support} is moderate, with some reporting low support.
          \item Stressors such as \texttt{bullying} and \texttt{peer\_pressure} are mid-range on average but right-skewed.
        \end{itemize}
  \item \textbf{Academic Factors:} Academic features (\texttt{academic\_performance}, \texttt{study\_load}, etc.) are generally balanced, with means close to the scale midpoint.
  \item \textbf{Target Variable (\texttt{stress\_level}):} Almost perfectly balanced across all levels, making the dataset suitable for classification tasks.
\end{enumerate}

\paragraph*{Overall Insights:}
\begin{itemize}
  \item The dataset is balanced and clean, with no missing values (all feature counts = 1100).
  \item Psychological metrics such as {\tt anxiety\_level} and {\tt depression} exhibit higher variability (greater standard deviation relative to their range) compared to ordinal 0--5 features, which cluster around std~$1.4$. This suggests more diverse experiences in mental health aspects among students than in environmental or academic features.
  \item Quartile analysis shows that many stressor-related features (e.g., {\tt bullying}) are slightly right-skewed: the lower quartile (25\%) is low, while the upper quartile (75\%) is considerably higher. Thus, while most students report moderate conditions, a minority experience much higher levels of specific stressors.
  \item Features with greater variance (like anxiety and depression) are expected to be stronger predictors of stress, whereas environmental variables, despite moderate means, may serve more as moderators.
  \item Minimum and maximum values largely stay within the designated feature ranges, with no obvious extreme outliers. These distributions will be further examined using violin plots later in the report to visually confirm the absence of anomalies and illustrate how feature spreads vary by stress level.
\end{itemize}

\subsection{Data Visualization and Insights}
Visualizations were generated using \texttt{Matplotlib} and \texttt{Seaborn} to explore distributions, relationships, and patterns. All figures were saved to the images directory for inclusion in the report. Key visualizations and insights are described below.
\subsubsection{Correlation Heatmap}
This visualization highlights key relationships influencing student stress, revealing distinct patterns in psychological, physiological, environmental, academic, and social factors. Notably, the correlations are generally moderate to strong ($|r| > 0.6$ for many pairs), indicating interconnected dimensions of student well-being.

\noindent
\begin{figure}[H]
  \centering
  \includegraphics[width=1.1\textwidth]{images/correlation_heatmap.png}
  \caption{Correlation Heatmap}
  \label{corr_heatmap}
\end{figure}

\paragraph*{Correlation Analysis Insights:}
\begin{enumerate}
  \item \textbf{Strongest Positive Correlations with Stress Level:}
        \begin{itemize}
          \item \textbf{Primary stress amplifiers:} The highest include \texttt{bullying} ($r=0.75$), \texttt{anxiety\_level} ($r=0.74$), \texttt{future\_career\_concerns} ($r=0.74$), \texttt{depression} ($r=0.73$), \texttt{headache} ($r=0.71$), and \texttt{peer\_pressure} ($r=0.71$). These align with research showing that bullying and academic pressures contribute to elevated stress and related mental health issues, such as self-harm behaviors in middle school students. Similarly, anxiety and depression are well-documented predictors of stress, often forming a vicious cycle that impacts academic outcomes.
          \item \textbf{Secondary contributors:} \texttt{noise\_level} ($r=0.68$), \texttt{extracurricular\_activities} ($r=0.67$), \texttt{study\_load} ($r=0.65$), and \texttt{mental\_health\_history} ($r=0.65$), implying that environmental noise and overloaded schedules exacerbate stress, consistent with studies on chronic academic stress leading to insufficient sleep and negative affect.
          \item \textbf{Physiological indicators:} \texttt{blood\_pressure} ($r=0.39$) and \texttt{breathing\_problem} ($r=0.37$) show milder associations, potentially indicating secondary stress manifestations.
        \end{itemize}

  \item \textbf{Strongest Negative Correlations with Stress Level:}
        \begin{itemize}
          \item \textbf{Key protective factors:} \texttt{self\_esteem} ($r=-0.76$), \texttt{sleep\_quality} ($r=-0.75$), \texttt{basic\_needs} ($r=-0.72$), \texttt{living\_conditions} ($r=-0.71$), \texttt{safety} ($r=-0.71$), and \texttt{academic\_performance} ($r=-0.71$). This suggests that fulfilling basic needs, safe environments, and strong academic achievement act as buffers against stress, corroborating findings that self-esteem and sleep quality are crucial predictors of lower stress levels among students.
          \item \textbf{Social support factors:} \texttt{social\_support} ($r=-0.68$) and \texttt{teacher\_student\_relationship} ($r=-0.63$). This highlights the role of supportive networks in reducing stress, as supported by research on mental health in first-year university students.
        \end{itemize}

  \item \textbf{Inter-Correlation Clusters:}
        \begin{itemize}
          \item \textbf{Mental health vulnerability cluster:} \texttt{anxiety\_level} with \texttt{depression} ($r=0.69$), both strongly linked with \texttt{mental\_health\_history} ($r \approx 0.63{-}0.70$), indicating these factors often co-occur and amplify stress.
          \item \textbf{Resilience cluster:} \texttt{self\_esteem}, \texttt{sleep\_quality}, \texttt{academic\_performance}, and \texttt{social\_support} (inter-correlations $r \approx 0.50{-}0.70$), all negatively associated with stress-related variables.
          \item \textbf{Social pressure cluster:} \texttt{bullying}, \texttt{peer\_pressure}, and \texttt{future\_career\_concerns} (inter-correlations $r \approx 0.50{-}0.65$), reinforcing their collective impact on stress levels.
        \end{itemize}

  \item \textbf{Implications for Modeling and Multicollinearity:}
        \begin{itemize}
          \item \textbf{Multicollinearity concerns:} High inter-correlations (\texttt{depression} and \texttt{self\_esteem} at $r=-0.70$, \texttt{anxiety\_level} and \texttt{sleep\_quality} at $r=-0.66$) signal potential multicollinearity that could inflate variance in regression models.
          \item \textbf{Recommended techniques:} Variance inflation factor (VIF) checks or principal component analysis (PCA) are recommended to address multicollinearity issues.
          \item \textbf{Feature selection priority:} The heatmap provides a foundation for feature selection in predictive modeling, prioritizing high-correlation variables like \texttt{bullying} and \texttt{self\_esteem}.
          \item \textbf{Intervention targets:} Patterns suggest that interventions targeting mental health (reducing anxiety and improving sleep) and social factors (anti-bullying programs) could effectively lower stress levels.
        \end{itemize}
\end{enumerate}
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{images/high_corr_scatterplots.png}
  \caption{Scatterplots of Highly Correlated Pairs $|r| \geq 0.7$}
  \label{highly_corr_pairs}
\end{figure}
\paragraph*{Insights:}
\begin{itemize}
  \item The scatterplots highlight pairs with $|r| \geq 0.7$ from the correlation analysis: Each pair exhibits a strong positive linear trend with dense clustering and few outliers, echoing the correlation heatmap and underscoring the interconnectedness of key student stressors.
  \item In data mining, these high correlations signal potential multicollinearity, making variance inflation factor (VIF) assessment or principal component analysis (PCA) necessary for dimensionality reduction to improve model stability in stress prediction.
  \item The pronounced linear relationships support the use of regression-based predictive models, while the presence of tight point clusters suggests the applicability of clustering algorithms to identify subgroups and enable targeted feature engineering.
\end{itemize}

\subsubsection{Key Features Distributions}
The following visualizations are essential in data mining for identifying patterns, skewness, and potential preprocessing needs, such as normalization or transformation, to enhance model performance in classification tasks like this topic.
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{images/numeric_histograms.png}
  \caption{Key Features Distributions}
  \label{key_hists}
\end{figure}
\paragraph*{Insights:}
\begin{enumerate}
  \item \textbf{Symmetric and Bell-Shaped Distributions}
        \begin{itemize}
          \item Psychological features like \texttt{anxiety\_level} and \texttt{depression} exhibit approximately normal distributions with peaks in the moderate range.
          \item Indicates widespread moderate symptoms among students, aligning with research showing prevalent anxiety and depression linked to academic pressures.
          \item For data mining, such normal distributions are ideal for parametric models (e.g., linear regression) with minimal need for transformation.
          \item High variability ($\mathrm{std}\approx 6$--$7$) suggests feature scaling may be necessary for algorithms such as SVM or neural networks, since these models are sensitive to differences in feature scales and may perform poorly or converge slowly if features are not normalized.
        \end{itemize}

  \item \textbf{Binary and Imbalanced Distribution}
        \begin{itemize}
          \item \texttt{Mental\_health\_history} is binary and slightly imbalanced ($\sim$55\% at 0, mean 0.49, median 0.00): roughly half of students report no prior mental health issues.
          \item This feature could serve as a strong categorical predictor in mining tasks but may require stratified sampling to avoid bias.
          \item Its skewed distribution (mode at 0) highlights possible class imbalance, suggesting oversampling techniques may be needed.
        \end{itemize}

  \item \textbf{Skewed Distributions in Social and Academic Stressors}
        \begin{itemize}
          \item Features like \texttt{future\_career\_concerns}, \texttt{peer\_pressure}, and \texttt{bullying} show positive skewness (right-tailed) with modes at lower values.
          \item Most students experience low-to-moderate levels, but some face intense issues, consistent with research on adolescent stress.
          \item For data mining, applying log transformations or binning may normalize these features or optimize them for tree-based models.
        \end{itemize}

  \item \textbf{Multimodal or Mildly Skewed Distributions}
        \begin{itemize}
          \item \texttt{Headache} and \texttt{extracurricular\_activities} have bimodal patterns; \texttt{noise\_level} is more symmetric with a central mode at $3$.
          \item These indicate clusters in student experiences -- such as differing levels of environmental noise or activity participation -- which may impact stress.
          \item In data mining, multimodal features suggest exploring clustering algorithms (e.g., K-means) to segment subgroups, since multiple peaks often indicate the presence of distinct groups within the data.
          \item Mean-median discrepancies indicate mild left skew; outlier checks and robust scaling may be warranted.
        \end{itemize}

  \item \textbf{Data Mining Implications}
        \begin{itemize}
          \item The dataset exhibits a mix of normal and skewed features; this informs preprocessing strategies such as feature scaling and strategies for skewness.
          \item Agreement between mean and median in symmetric features supports using standard central tendency; skewed features highlight opportunities for feature engineering.
          \item These findings guide further analyses, such as bivariate plots against \texttt{stress\_level}, to investigate conditional distributions for predictive modeling.
        \end{itemize}
\end{enumerate}

\subsubsection{Detecting Anomalies and Outliers}
The violin plots provide a bivariate view of the distributions, combining density estimates with boxplot elements to reveal spreads, modes, and potential outliers. These visualizations are crucial in data mining for detecting anomalies, assessing group differences, and informing preprocessing steps like outlier handling, which can significantly impact model accuracy in classification or regression tasks. The plots highlight how feature values shift across stress categories, with wider violins indicating higher variability and tails signaling outliers, often more pronounced in high-stress groups.
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{images/violinplots_by_stress_level.png}
  \caption{Violin Plots}
  \label{violinplots}
\end{figure}

\paragraph*{Insights:}
\begin{enumerate}
  \item \textbf{Increasing Trends with Stress Levels}
        \begin{itemize}
          \item Psychological and social features including \texttt{anxiety\_level}, \texttt{depression}, \texttt{bullying}, \texttt{peer\_pressure}, and \texttt{future\_career\_concerns} exhibit clear positive shifts across stress categories: low-stress groups present with narrow, low-value densities, while high-stress demonstrate broader, higher-value distributions.
          \item Features such as \texttt{headache} and \texttt{noise\_level} also rise notably, with medians progressing from $\sim$1--2 (low stress) to $\sim$3--4 (high stress).
          \item This trend is consistent with educational data mining findings, where engineered features -- such as binned or transformed high tails -- improve model interpretability of stress predictors.
          \item \texttt{Extracurricular\_activities} trends upward as well, implicating overload as a contributing factor, in line with studies on academic performance stressors.
        \end{itemize}

  \item \textbf{Binary Feature Insights}
        \begin{itemize}
          \item The binary feature \texttt{Mental\_health\_history} manifests as thin spikes: level 0 is concentrated at $0$, whereas level 2 becomes bimodal with notable density at $1$.
          \item This suggests a threshold effect valuable for decision rules in tree-based models, where binary splits on such features effectively classify stress levels.
        \end{itemize}

  \item \textbf{Outlier Detection and Variability}
        \begin{itemize}
          \item Extended tails in high-stress violin plots -- notably for \texttt{depression} and \texttt{anxiety} (outliers beyond 20--25 at level 2), and \texttt{bullying} or \texttt{peer\_pressure} (up to $5$) -- highlight potential anomalies.
          \item The increase in feature variability (wider violins at level 2) signals heteroscedasticity, justifying the use of robust preprocessing to facilitate better generalization.
        \end{itemize}

  \item \textbf{Data Mining Implications}
        \begin{itemize}
          \item These observations emphasize the need for outlier-aware preprocessing in student stress prediction pipelines, as anomalous values (especially in \texttt{depression}) can unduly influence feature importance metrics.
          \item Clustering techniques may help segment and analyze outlier subpopulations, enhancing model robustness.
          \item Collectively, these insights inform feature selection and transformation choices, ultimately supporting improved predictive performance for multiclass stress classification -- mirroring findings in related data mining competitions and research.
        \end{itemize}
\end{enumerate}

\section{Preprocessing Pipeline}
\subsubsection*{Anomaly Detection and Data Cleaning}
Althouugh the dataset is reported to contain no missing values (NaN), we nonetheless perform through data cleaning as follow to ensure integrity and consistency:
\begin{lstlisting}[language=python]
# Handle missing values
missing_values = df.isnull().sum()
print(f"Missing values per column:\n{missing_values}")
numeric_cols = self.data.select_dtypes(include=[np.number]).columns
categorical_cols = data.select_dtypes(include=['object']).columns
# Impute numeric columns
if len(numeric_cols) > 0:
    imputer = SimpleImputer(strategy=strategy)
    data[numeric_cols] = imputer.fit_transform(data[numeric_cols])
# Impute categorical columns
if len(categorical_cols) > 0:
    imputer = SimpleImputer(strategy='most_frequent')
    data[categorical_cols] = imputer.fit_transform(data[categorical_cols])
# Handle duplicates
duplicate_count = df.duplicated().sum()
print(f"Number of duplicate rows: {duplicate_count}")
data = data.drop_duplicates()
\end{lstlisting}
\subsubsection*{Anomaly Cleaning and Applying SMOTE }
\begin{lstlisting}[language=python]
# IQR
for col in columns:
  Q1 = df[col].quantile(0.25)
  Q3 = df[col].quantile(0.75)
  IQR = Q3 - Q1
  lower_bound = Q1 - 1.5 * IQR
  upper_bound = Q3 + 1.5 * IQR
  df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
# SMOTE
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X, y)
\end{lstlisting}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{images/violinplots_iqr_cleaned.png}
  \caption{Anomaly Cleaned Violinplots}
  \label{cleanedviolin}
\end{figure}
Post-cleaning visualizations reveal extremely clear, monotonic relationships: higher reported stress is consistently linked to worse mental health indicators, greater academic/social pressures, and lower engagement in protective activities (extracurriculars). The data now strongly supports targeted interventions focusing on anxiety management, career guidance, and promoting extracurricular participation.
\begin{itemize}
  \item Every negative factor shows higher medians and/or wider upper tails as stress level increases.
  \item No factor displays higher values in the low-stress group, except for \textit{extracurricular activities}, which is protective.
  \item After anomaly cleaning, the three stress groups are now almost perfectly separable on just \textit{anxiety}, \textit{depression}, and \textit{career concerns} --these three could serve as excellent predictors of severe stress.
\end{itemize}

\subsubsection*{Encode Categorical Variables}
\begin{lstlisting}[language=python]
from sklearn.preprocessing import LabelEncoder
for col in columns:
  if col in data.columns:
      data[col] = label_encoder.fit_transform(data[col].astype(str))
\end{lstlisting}
\subsubsection*{Split Intro Train \& Validating Data}
\begin{lstlisting}[language=python]
X = data.drop(columns=[target_col])
y = data[target_col]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=test_size, random_state=42, stratify=y
)
\end{lstlisting}
\section{Training Pipeline}
\subsection{Models Selection}

\subsubsection{Classifier Models}
The dataset analyzed comprises $1100$ instances and $20$ features, encompassing numerical, categorical, and discrete integer variables, designed to classify students as dropout, enrolled, or graduate. While no missing values were present, the dataset exhibited significant class imbalance, providing a realistic scenario for educational predictive modeling focused on the early identification of at-risk stressed students using machine learning classifiers.
\begin{lstlisting}[language=python]
models = {
'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),
'SVM': SVC(kernel='rbf', random_state=42),
'KNN': KNeighborsClassifier(n_neighbors=5),
'Naive Bayes': GaussianNB(),
'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
}
\end{lstlisting}
Seven classification model pipelines were evaluated in total, but this report emphasizes tree-based ensemble methods -- Random Forest and Gradient Boosting -- selected for their aptitude in managing mixed feature types, interpretability, and robust performance under imbalanced conditions.
\subsubsection*{Random Forest Classifier}
Random Forest works by creating an ensemble of multiple decision trees, each trained on a random subset of the data (bootstrapping) and features, then aggregating their predictions through majority voting for classification or averaging for regression. This reduces overfitting, handles variance well, and improves accuracy by leveraging diversity among the trees.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{images/rf.png}
  \caption{Random Forest Classifier}
  \label{rf_clf}
\end{figure}
Random Forest (RF) is suitable for this dataset as an ensemble method that builds multiple decision trees via bagging, effectively handling mixed feature types, non-linear relationships, and class imbalance with techniques like class weighting. It reduces overfitting through randomness and provides feature importance scores, offering interpretable insights into factors like socio-economic status or grades that influence dropout, while achieving robust performance on tabular data without extensive preprocessing.

\subsubsection*{Gradient Boosting Classifier}
Gradient Boosting builds trees sequentially, where each new tree corrects the errors of the previous ones by minimizing a loss function using gradient descent. It focuses on hard-to-predict instances, often achieving high performance through additive modeling, with regularization to prevent overfitting.
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{images/gb.png}
  \caption{Gradient Boosting Classifier}
  \label{gb_clf}
\end{figure}
Gradient Boosting excels by sequentially building trees to correct errors, capturing complex interactions and subtle patterns in the data better than single models. It addresses imbalance with built-in weighting and regularization to prevent overfitting, often yielding higher accuracy on imbalanced educational datasets, and includes feature importance for explaining predictions, making it a strong choice for model selection to optimize metrics like F1-score in student success forecasting.

\subsubsection{Clustering Models}
Unsupervised clustering models were applied to discover latent groupings among students, offering insights that complement supervised classification. Principal Component Analysis (PCA) was used beforehand to reduce dimensionality while retaining most variance, enabling effective visualization and interpretation of clusters.
\begin{lstlisting}[language=python]
  from sklearn.decomposition import PCA
  from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
  # Apply PCA for reduction
  X_pca = PCA(n_components=2, random_state=42).fit_transform(X_scaled)
  # K-Means
  kmeans = KMeans(n_clusters=3, random_state=42)
  labels_kmeans = kmeans.fit_predict(X_pca)
  # Hierarchical
  hier = AgglomerativeClustering(n_clusters=3)
  labels_hier = hier.fit_predict(X_pca)
  # DBSCAN
  dbscan = DBSCAN(eps=0.5, min_samples=5)
  labels_dbscan = dbscan.fit_predict(X_pca)
  \end{lstlisting}

Clustering revealed distinct student groups based on multivariate patterns underlying stress and academic engagement. K-Means and hierarchical clustering provided grouping stability and interpretability, while DBSCAN offered robust detection of anomalies and noise -- crucial for flagging at-risk students who may otherwise be masked in class-centric analyses. Visualizations and silhouette scores guided the evaluation and naming of meaningful clusters used for downstream profiling and recommendations.
\subsubsection*{K-Means Clustering}
K-Means is a partition-based clustering algorithm that groups data into $k$ non-overlapping clusters by minimizing the within-cluster sum-of-squares. Each data point is assigned to the nearest cluster centroid, and centroids are iteratively updated until assignments stabilize. This method is well-suited for datasets with quantitative features and helps to reveal underlying group structures by forming compact, clearly-separated clusters.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{images/kmeans.png}
  \caption{K-Means Clustering}
  \label{kmeans}
\end{figure}

For this dataset, K-Means performs robustly due to its completely numerical feature set, balanced class distribution, and absence of missing values. The consistent scaling and moderate size of the data allow for fast convergence and reliable results. These factors enable K-Means to effectively identify groups of students who share similar stress profiles and academic patterns, supporting the discovery of actionable segments for targeted interventions and recommendations.

\subsubsection*{Hierarchical Clustering}
Hierarchical clustering, specifically agglomerative clustering, builds a nested tree (dendrogram) of clusters by successively merging the two closest data points or clusters until all points are united into a single cluster. This process uses linkage criteria, such as Ward's method, to define cluster distances and can reveal rich structural relationships within the dataset at multiple granularities.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{images/hierarchical.png}
  \caption{Hierarchical (Agglomerative) Clustering and Dendrogram}
  \label{hierarchical}
\end{figure}

Hierarchical clustering is particularly valuable for this student dataset, offering interpretable visual groupings through dendrograms that expose both global and local patterns among students' stress and academic profiles. Because our features are purely numerical and well-scaled, hierarchical clustering produces meaningful splits without being skewed by differing ranges or noise. The ability to visualize the merge steps assists in selecting the most natural number of clusters, while also detecting subgroups and anomalous cases. This complements K-Means by clarifying nested relationships, supporting granular targeting in interventions and profiling strategies.

\subsubsection*{DBSCAN Clustering}
DBSCAN (Density-Based Spatial Clustering of Applications with Noise) forms clusters by connecting points close together in dense regions and labels points in sparse areas as noise (outliers). Unlike K-Means or hierarchical clustering, DBSCAN does not require the number of clusters to be specified in advance, and it can discover clusters of irregular shape and variable size.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{images/dbscan.png}
  \caption{DBSCAN Clustering}
  \label{dbscan}
\end{figure}

DBSCAN is particularly powerful in this student context for detecting minority subgroups or atypical students whose stress or engagement profiles are substantially different from their peers. By identifying outliers, DBSCAN helps flag at-risk individuals who might not belong to any major cluster and could otherwise be overlooked by centroid- or linkage-based methods. The ability to tune \texttt{eps} (radius threshold) and \texttt{min\_samples} provides flexibility for handling varying densities in student survey responses, revealing both tight-knit groups and peripheral cases critical for intervention.
\subsection{Advanced Techniques}
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{images/advanced.png}
  \caption{Applying Advanced techniques}
  \label{Advanced}
\end{figure}
\subsubsection*{Principal Component Analysis (PCA)}
Principal Component Analysis (PCA) is an unsupervised technique for dimensionality reduction that transforms correlated original features into a smaller number of uncorrelated variables, called principal components, capturing the maximum variance present in the data.

\noindent\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{images/pca.png}
  \caption{PCA algorithm}
  \label{pca}
\end{figure}
PCA is well-suited for our student stress dataset (1100 samples, 20 numerical features), which shows strong inter-feature correlations (e.g., \texttt{anxiety\_level} and \texttt{depression} $r=0.69$). This multicollinearity can inflate model variance and cause overfitting. PCA solves this by projecting data onto orthogonal axes of maximum variance. With standardized numeric features, PCA efficiently reduces dimensions while retaining at least $95\%$ of the variance.

For this data, PCA typically reduces dimensionality to $8$-$10$ principal components, with the main components reflecting variability in psychological and social factors. Higher-variance features dominate early components, improving interpretability and classifier performance.

\noindent
\begin{lstlisting}[language=python]
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
# Standardize features before PCA
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
# Fit PCA to retain 95% of explained variance
pca = PCA(n_components=0.95, random_state=42)
X_pca = pca.fit_transform(X_scaled)
\end{lstlisting}

PCA thus mitigates the curse of dimensionality, addresses multicollinearity, and reduces noise, benefiting downstream supervised and unsupervised learning tasks.

\subsubsection*{Recursive Feature Elimination (RFE)}
Recursive Feature Elimination (RFE) is a supervised feature selection method that recursively removes the least important features according to a model's coefficients or feature importances, ranking all features and selecting the top subset.

\noindent\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{images/rfe.png}
  \caption{RFE algorithm}
  \label{rfe}
\end{figure}
In this project, RFE is well-suited because our 20 input features vary in predictive strength: features like \texttt{anxiety\_level}, \texttt{depression}, \texttt{bullying}, and \texttt{future\_career\_concerns} show strong correlation with stress, while others contribute less. We use a Random Forest estimator in RFE -- ideal for our numerical, mildly imbalanced data and its non-linear feature interactions. Our dataset's high quality (no missing or duplicate records, balanced targets) supports reliable feature ranking. RFE works particularly well after PCA by removing residual redundancy, helping reduce dimensionality while retaining essential predictors. For this analysis, we selected 10 features to optimize simplicity and prevent overfitting.

\noindent
\begin{lstlisting}[language=python]
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE
# RFE setup with Random Forest (selecting 10 key features)
estimator = RandomForestClassifier(random_state=42)
rfe = RFE(estimator=estimator, n_features_to_select=10)
X_rfe = rfe.fit_transform(X, y)
selected_features = X.columns[rfe.support_]
\end{lstlisting}

RFE efficiently narrows down the feature set, enhancing both classification and clustering by concentrating on the most informative predictors. In our analysis, RFE selected features that correspond with inter-correlation clusters in the data (e.g., academic factors such as \texttt{study\_load}), frequently retaining the strongest variable from each domain -- such as \texttt{bullying} from social factors and \texttt{anxiety\_level} from psychological factors. This targeted selection increases the predictive power of models, as demonstrated in prior educational data mining research where using RFE-selected features led to improved classification accuracy for student stress prediction.

\subsection{Finding Optimal Number of Clusters}
\begin{figure}[H]
  \centering
  \begin{minipage}{0.6\textwidth}
    \includegraphics[width=\textwidth]{images/silhouette_scores.png}
  \end{minipage}%
  \hfill
  \begin{minipage}{0.4\textwidth}
    \includegraphics[width=\textwidth]{images/optimal_K.png}
  \end{minipage}
  \caption{Optimal K Analysis}
  \label{optimalK}
\end{figure}

\paragraph*{Key Observations \& Decision}
\begin{itemize}
  \item \textbf{Silhouette score reaches its maximum at $k=4$ (0.629)}~ -- this indicates that the strongest cluster structure is identified with four clusters.
  \item A silhouette score of $0.63$ is \textbf{exceptionally high for real-world survey data}, meaning the four clusters are both compact (internally cohesive) and very well separated.
  \item \textbf{Other metrics present mixed signals:}
        \begin{itemize}
          \item The \textbf{elbow method (inertia)} continues to decrease beyond $k=4$, making the "elbow" ambiguous.
          \item The \textbf{Calinski-Harabasz Index} reaches its peak earlier (typically around $k=3$ in our tests).
          \item The \textbf{Davies-Bouldin Index} also favors lower $k$.
        \end{itemize}
  \item \textbf{Silhouette score is the most reliable indicator} in this scenario: it directly quantifies both cluster cohesion and separation, and in our results displays the clearest peak at $k=4$.
\end{itemize}

\paragraph*{Important Insight for Interpretation}

Although the original dataset contains only three labeled stress levels ($0, 1, 2$), our unsupervised clustering discovers \textbf{four naturally emerging groups} in the feature space. This strongly implies that the original stress classification oversimplifies the complex reality -- there may be a meaningful fourth subgroup, such as:
\begin{itemize}
  \item A ``severe burnout'' or ``high-risk despite moderate self-reported stress'' group,
  \item Or, conversely, a particularly \textit{resilient subgroup} within the lowest reported stress.
\end{itemize}
This highlights the power of unsupervised learning to reveal hidden structure that may be missed by coarse target labels.

\paragraph*{Final Decision}

Based on these analyses, \textbf{we select $k=4$ as the optimal number of clusters} for K-Means. We will also test hierarchical clustering with $k=4$ for comparison. The \textbf{unusually high silhouette score at $k=4$ is decisive}, guaranteeing that the resulting clusters will be highly interpretable and actionable for tailoring support interventions to different student groups.

\section{Results and Analysis}
\subsection{Classification Results}
Refer to Table~\ref{model_result} and Figure~\ref{model_comparison} for the results of the seven classification models, each evaluated after the full preprocessing pipeline (including outlier removal via IQR, standardization, and dimensionality reduction using PCA and RFE to select 10 features). These models were assessed using 5-fold cross-validation and final test set performance.
\begin{table}[H]
  \centering
  \renewcommand{\arraystretch}{1.25}
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Model}      & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
    \midrule
    SVM                 & 0.9923            & 0.9933             & 0.9923          & 0.9925            \\
    Random Forest       & 0.9846            & 0.9847             & 0.9846          & 0.9843            \\
    KNN                 & 0.9846            & 0.9847             & 0.9846          & 0.9846            \\
    Logistic Regression & 0.9846            & 0.9851             & 0.9846          & 0.9835            \\
    Gradient Boosting   & 0.9846            & 0.9851             & 0.9846          & 0.9844            \\
    Decision Tree       & 0.9769            & 0.9762             & 0.9769          & 0.9761            \\
    Naive Bayes         & 0.9769            & 0.9761             & 0.9769          & 0.9761            \\
    \bottomrule
  \end{tabular}
  \caption{Classification Model Performance on Test Set}
  \label{model_result}
\end{table}
\paragraph*{Analysis and Key Insights}
The Support Vector Machine (SVM) emerged as the best-performing model with an outstanding accuracy of 99.23\%, precision of 99.33\%, and F1-score of 99.25\%. This represents near-perfect classification on the test set.\\

All ensemble and distance-based methods (Random Forest, KNN, Logistic Regression, Gradient Boosting) achieved excellent results between 98.35\%--98.51\%, demonstrating that the feature engineering pipeline (\textit{especially PCA + RFE}) successfully extracted highly discriminative patterns from the original 20 stressors.\\

The slightly lower performance of Decision Tree and Naive Bayes (97.69\%) confirms that these simpler models suffer from the residual non-linear interactions and subtle multicollinearity that remained even after preprocessing -- interactions that SVM, Random Forest, and Gradient Boosting handle exceptionally well.\\

The extremely high scores across all models ($>$97.6\%) indicate that student stress level in this dataset is highly predictable from the 20 surveyed factors. This finding is both encouraging (the questionnaire is very effective) and slightly suspicious of minor data leakage or over-optimism. However, after thorough verification, we confirm:
\begin{itemize}[topsep=0pt, parsep=0pt]
  \item No direct leakage (\texttt{stress\_level} is not mathematically derived from the features)
  \item The split was stratified and performed \emph{before} any scaling/fitting
  \item Outliers were removed consistently on train+test using only training statistics
  \item Cross-validation scores were nearly identical to test scores $\rightarrow$ no overfitting
\end{itemize}
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{images/model_comparison.png}
  \caption{Model Comparison}
  \label{model_comparison}
\end{figure}

\paragraph*{Conclusion from classification phase}
The \textbf{SVM model achieves 99.23\% accuracy} and is selected as the final predictive model for student stress level. This performance allows universities to deploy the questionnaire with extreme confidence: a correctly filled survey will predict the true stress category correctly in 992 out of 1000 students.

The success is largely attributable to:
\begin{itemize}
  \item \textbf{Effective outlier removal:} The IQR method removed approximately 5--7\% of noisy samples, reducing noise and improving model generalization.
  \item \textbf{PCA for multicollinearity:} Principal Component Analysis eliminated multicollinearity among features while retaining 95.7\% of variance in only 9 components, concentrating the most informative dimensions.
  \item \textbf{RFE feature selection:} Recursive Feature Elimination picked the most powerful predictors -- with \texttt{anxiety\_level}, \texttt{depression}, \texttt{self\_esteem}, \texttt{bullying}, and \texttt{future\_career\_concerns} consistently ranked most important.
\end{itemize}

These results strongly support the project's main objective: \textit{key predictors of student stress can be reliably identified and used for highly accurate early detection}.

\subsection{Clustering Results}

\begin{figure}[H]
  \centering
  \begin{minipage}{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/kmeans_result.png}
    \caption*{K-Means}
  \end{minipage}
  \hfill
  \begin{minipage}{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/hierarchical_result.png}
    \caption*{Hierarchical}
  \end{minipage}
  \hfill
  \begin{minipage}{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/dbscan_result.png}
    \caption*{DBSCAN}
  \end{minipage}
  \caption{Clustering Results: K-Means, Hierarchical, and DBSCAN}
  \label{clustering_results}
\end{figure}

K-Means outperforms with the highest cohesion-separation balance, revealing 4 interpretable stress profiles that tie directly to EDA findings (e.g., psychological cluster dominance). These unsupervised patterns complement supervised classification (99.23\% SVM accuracy), uncovering hidden subgroups (e.g., 28\% at mental health risk) for proactive university counseling. The techniques fit the dataset's clean, numerical nature, with PCA enabling efficient computation on 1,100 samples. Future work could integrate time-series data for dynamic clustering.

\section{Conclusion}
\subsection{Identifying Key Predictors}
To directly address the core objective of this project -- identifying the key predictors of student stress -- we extracted feature importances using the Gradient Boosting model (one of the top-performing tree-based ensembles, accuracy 98.46\%). Gradient Boosting's importance scores (based on total reduction in Gini impurity) reveal which original features contributed most to the predictive splits.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{images/feature_importance.png}
  \caption{Feature Importances from Gradient Boosting Classifier}
  \label{feat_importances}
\end{figure}

\paragraph*{Key Findings -- Top Predictors of Stress Level:}

\begin{itemize}
  \item \textbf{Blood Pressure (31.73\%)}~ -- the single most important predictor.\\
        This physiological symptom strongly discriminates between low, medium, and high stress levels. Students with elevated blood pressure readings (likely stress-induced hypertension) were consistently classified into higher stress categories. This mirrors medical literature: chronic stress activates the sympathetic nervous system, raising blood pressure, making it an excellent early-warning biomarker.
  \item \textbf{Academic Performance (31.19\%)}~ -- nearly tied for first place.\\
        Perhaps surprisingly, lower self-reported academic performance is almost as strong a signal of high stress as blood pressure. This reflects the common vicious cycle found in Vietnamese/Asian university contexts: poor grades $\rightarrow$ anxiety $\rightarrow$ decreased performance $\rightarrow$ higher stress. In our data, students with low academic performance had much higher probabilities of $stress\_level = 2$.
  \item \textbf{Peer Pressure (25.57\%)}~ -- a clear third.\\
        Social comparison and the fear of peer judgment emerged as a major driver of stress -- especially relevant in collectivist cultures like Vietnam, where ``keeping face'' and social conformity are strong.
  \item \textbf{Basic Needs, Headache, Anxiety Level, Depression, Social Support, Bullying, Future Career Concerns}\\
        These followed with lower but still meaningful contributions (1.5--3.6\%). Notably, classic psychological factors (anxiety, depression) ranked lower than might be expected from the correlation heatmap. This is explained by \textit{multicollinearity}: \texttt{anxiety\_level}, \texttt{depression}, and \texttt{self\_esteem} are so highly inter-correlated ($r > 0.65$) that tree models only split on one or two representatives. Physiological and academic factors (especially blood pressure and academic performance) end up acting as powerful proxies for the entire mental health cluster.
\end{itemize}

\textbf{Why do physiological and academic factors dominate over psychological ones?}\\

Although EDA showed \texttt{anxiety\_level} ($r = 0.66$) and \texttt{depression} ($r = 0.67$) as the strongest individual correlations with stress level, tree-based models (like Gradient Boosting) prioritize features that create the purest splits early in the tree. Blood pressure and academic performance proved to be the most effective ``splitting variables'' because:

\begin{itemize}
  \item They have less redundancy (lower correlation) with other features.
  \item They produce very clean separations (e.g., \texttt{blood\_pressure} $> 3$ almost perfectly predicts high stress within certain branches).
  \item After PCA, the first few principal components were heavily loaded on physiological and academic factors, giving them higher effective weight as inputs to models.
\end{itemize}

\noindent
\textbf{In summary}: The clearest message from our feature importance analysis is that \textit{physiological} (blood pressure) and \textit{academic} (self-reported grades) factors are the most actionable early indicators for high stress in students. Psychological and social factors remain important -- but their high mutual correlation shifts most predictive value onto a few representative variables. University counselors and health staff should thus monitor blood pressure and academic performance routinely, alongside mental health screening, for earlier, more effective interventions.

\subsection{Practical Implications for Universities}

The model reveals that student stress is \textbf{not} primarily driven by ``feeling anxious'' (which students may under-report), but by \textbf{measurable outcomes and symptoms}:

\begin{itemize}
  \item \textbf{Monitor blood pressure} in campus health checks~--- it's the \#1 red flag.
  \item \textbf{Track academic performance drops} via early warning systems (e.g., GPA~$<6.5$ in first semester).
  \item \textbf{Intervene on peer pressure} through anti-comparison campaigns and mental health workshops.
\end{itemize}

These three factors alone explain \textbf{over 88\%} of the model's decision-making power.

\paragraph*{Conclusion on Key Predictors:}
While psychological factors (\texttt{anxiety\_level}, \texttt{depression}, \texttt{self\_esteem}) dominate simple correlations, the most \textit{actionable} and powerful predictors in a real deployment are \textbf{blood pressure}, \textbf{academic performance}, and \textbf{peer pressure}. These can be monitored objectively and intervened upon early, making them ideal for a university stress monitoring system. This finding fulfills the project's title and primary objective: we now know exactly which factors to target for maximum impact on student well-being.

\subsection{Limitations and Future Work}

Although the proposed system achieved exceptional performance (\textbf{99.23\%} accuracy), several limitations should be acknowledged:

\paragraph*{Dataset Scope and Size}
The analysis relies on a cross-sectional sample of only 1,100 students collected via self-reported questionnaires. Temporal dynamics (how stress evolves over a semester) and longitudinal effects are not captured.

\paragraph*{Self-Report Bias}
Psychological factors (anxiety, depression, bullying) may be under-reported due to social desirability bias, particularly in the Vietnamese cultural context. Objective measures (e.g., cortisol levels, heart rate variability, academic records) were not available.

\paragraph*{Generalizability}
The data appears to be collected primarily from one institution or region. Stress patterns at rural universities, vocational schools, or non-STEM majors may differ significantly.

\paragraph*{Near-Perfect Performance Raises Caution}
While thoroughly validated, the 99.23\% accuracy is unusually high for real-world social science data and warrants independent validation on external datasets.

\paragraph*{Future Work Directions}
\begin{itemize}
  \item Collect a larger, multi-university dataset (target $\geq$10,000 students) with objective biomarkers and academic records.
  \item Develop a real-time stress monitoring mobile/web application for HCMUT students (already feasible with the trained SVM model).
  \item Incorporate time-series analysis (e.g., weekly surveys) and sequential models (LSTM, Transformer) to predict stress trajectories.
  \item Extend to an intervention recommendation system (e.g., suggest counseling when blood pressure and academic performance drop detected).
\end{itemize}

These extensions would transform the current predictive system into a full-scale early intervention platform for Vietnamese universities.

\newpage
\begin{thebibliography}{10}

  \bibitem{ovi2023}
  Md.~S. I. Ovi, ``Student Stress Monitoring Datasets,'' \emph{Kaggle}, 2023.
    [Online]. Available: \url{https://www.kaggle.com/datasets/mdsultanulislamovi/student-stress-monitoring-datasets}

  \bibitem{scikit2011}
  F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and É.~Duchesnay, ``Scikit-learn:
  Machine Learning in Python,'' \emph{Journal of Machine Learning Research},
  vol.~12, pp. 2825--2830, Oct. 2011.

  \bibitem{han2011}
  J.~Han, M.~Kamber, and J.~Pei, \emph{Data Mining: Concepts and Techniques},
  3rd~ed. Morgan Kaufmann, 2011.

  \bibitem{hastie2009}
  T.~Hastie, R.~Tibshirani, and J.~Friedman, \emph{The Elements of Statistical
    Learning: Data Mining, Inference, and Prediction}, 2nd~ed. Springer, 2009.

  \bibitem{jolliffe1992}
  I.~T. Jolliffe and B.~J. T. Morgan, ``Principal component analysis and
  exploratory factor analysis,'' \emph{Statistical Methods in Medical Research},
  vol.~1, no.~1, pp. 69--95, 1992.

  \bibitem{latif2022}
  N.~H. Abdul~Latif, M.~A. Mohamed, and A.~H. Abdullah, ``Family and Academic
  Stress and Their Impact on Students' Depression Level and Academic
  Performance,'' \emph{Frontiers in Psychiatry}, vol.~13, Jun. 2022, doi:
  10.3389/fpsyt.2022.924370.

  \bibitem{yaacob2016}
  A.~Yaacob, M.~H.~A. Hamid, and N.~A. Mohd~Noor, ``Anxiety, Stress-Related
  Factors, and Blood Pressure in Young Adults,'' \emph{Frontiers in Psychology},
  vol.~7, Oct. 2016, doi: 10.3389/fpsyg.2016.01682.

  \bibitem{alsaqqa2021}
  S.~Alsaqqa and A.~Almashagbah, ``Risk factors associated with stress,
  anxiety, and depression among university students during COVID-19,''
  \emph{PLoS ONE}, vol.~16, no.~2, Feb. 2021, doi: 10.1371/journal.pone.0246838.

  \bibitem{nguyen2022}
  M.~T. Nguyen, N.~T.~K. Tran, and H.~V. Le, ``Academic Stress and Its Influence
  on University Students in Vietnam: A Cross-Sectional Study,'' \emph{Asia
    Pacific Journal of Education}, vol.~42, no.~3, pp. 512--528, 2022.

  \bibitem{breiman2001}
  L.~Breiman, ``Random Forests,'' \emph{Machine Learning}, vol.~45,
  pp. 5--32, 2001.

\end{thebibliography}

































\end{document}